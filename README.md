Implemented and analyzed core optimization algorithms including Batch Gradient Descent, Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, Momentum, RMSProp, and Adam from scratch using NumPy.

Built the complete training pipeline: parameter initialization, forward propagation, cost computation, backpropagation, and parameter updates.

Studied the effect of learning rate, mini-batch size, and exponential moving averages on convergence speed and training stability.

Compared optimization methods empirically using loss curves and observed faster convergence and smoother training with Adam and Momentum compared to vanilla gradient descent.

Applied optimization techniques to improve training efficiency of deep neural networks and reduce oscillations during learning.
